---
title: "TimeSeries2"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(tidyverse)
library(urca)
```

# Work with git

```{terminal, eval=FALSE}
# sync with the latest version
# Everytime before you start working, please sync with the latest version first.
# run this two lines of code in terminal line by line (seperately)

git fetch upstream
git merge upstream/master

# Then please read the message which will be sent back
```

# Imported data

```{r}
dat = read.csv("B3_HWA2.csv", stringsAsFactors = FALSE)
```

```{r}
# we want 400, and leave 4 out for forecasting
datm <- dat[1:400, ] # subset 400 first rows of dat, include all columns
datf <- dat[401:404, ] # subset last 4 rows of dat, include all columns

# process 1-5
Y1 <- as.ts(datm[ , 3])
Y2 <- as.ts(datm[ , 4])
Y3 <- as.ts(datm[ , 5])
y_4 <- as.ts(datm[ , 6])
y_5 <- as.ts(datm[ , 7])

```


## Task 1

### y_4

Here we are going to analyze the dataset y_4. As with the other datasets, we will use the Box-Jenkins approach.

#### Indentification

```{r}
y_4 <- as.ts(datm[ , 6])

## y_4 - AR(2)
layout(matrix(c(1, 1, 2,
                1, 1, 3), nrow=2, byrow=TRUE)) 
ts.plot(y_4, main = "Time Series y_4") # time series plot
acf(y_4, lag.max = 20, type = "correlation", plot = T, main = "ACF") # ACF
acf(y_4, lag.max = 20, type = "partial", plot = T, main = "PACF") # PCAF
par(mfrow = c(1,1)) # set plot window to default

```

Looking at our model, we can see that it looks centered around zero. It  looks fairly consistent in mean and variance. The ACF seems to be declining in a geometric fashion. The PACF has two "fingers". This leads us to suspect this is a AR-process of the second order, or AR(2).

#### Estimation

```{r}
### y_4
(m4 <- arima(y_4, order = c(2,0,0)))
sigma24 <- m4$sigma2
```


We estimate the parameters for the AR(2) process:

$$\phi_{1} = -0.282$$
$$\phi_{2} = 0.568$$
$$Var(\phi_{1}) = Var(\phi_{2}) = (0.041)^2$$
$$\sigma^2 = 0.9271$$

So our model is $$Y_{4} = 0.0174-0,282Y_{t-1} + 0.568Y_{t-2} + e_t$$

#### Evaluation

We begin by extracting the residuals. When we have done this we plot the residuals to check if the model has managed to capture the systematic variation.


```{r}
E4 <- residuals(m4)
```

```{r}
## y_4
layout(matrix(c(1, 1, 2,
                1, 1, 3), nrow=2, byrow=TRUE)) 
ts.plot(E4, ylab = "Residuals", col = "blue", main = "Residuals from Fitted Model y_4")
abline(a = mean(E4), b = 0) # adds horizonta2l line with mean(E4) as intercept and 0 slope
abline(a = mean(E4) + sigma24, b = 0, lty="dotted") # same as above + sigma2
abline(a = mean(E4) - sigma24, b = 0, lty="dotted") # same as above - sigma2

acf(E4, lag.max = 20, type = "correlation", plot = T, main = "ACF") # ACF
acf(E4, lag.max = 20, type = "partial", plot = T, main = "PACF") # PCAF
par(mfrow = c(1,1)) # set plot window to default

```

The plot of the residuals looks like white noise, with no pattern or trend. Both the ACF and PACF are equal to zero, which they should be since the residuals are randomness. Does, it seems like we have managed to separate the systematic and random variation.

```{r}
layout(matrix(c(1, 1, 2,
                1, 1, 3), nrow=2, byrow=TRUE)) 
ts.plot(y_4, E4, ylab = "Residuals", col = c("red","black"), main = "Residuals from Fitted Model y_4")
abline(a = mean(E4), b = 0) # adds horizonta2l line with mean(E4) as intercept and 0 slope
abline(a = mean(E4) + sigma24, b = 0, lty="dotted") # same as above + sigma2
abline(a = mean(E4) - sigma24, b = 0, lty="dotted") # same as above - sigma2
```

Here we can see the plot of y_4 in red, with the plot for the residuals superimposed on top, in black. We can see that the residuals do share all their patterns and trends with the plot for y_4.

```{r}
qqnorm(E4)
```

The QQ-plot looks to follow the theoretical values fairly well.

```{r}
(e4_acf <- acf(E4, lag.max = 20, type = "correlation", plot = F)) 
```

These are the autocorrelations for y_4, lag 0-20.

```{r}
Box.test(E4, type="Ljung-Box")
```

The p-value of the Box-Ljung test is 0.9317 > 0.05. Thus we can assume that not all autocorrelations are equal to zero for model y_4.

#### Forecast

```{r}
(y_4_pred <- predict(object = m4, n.ahead = 4))
```

Here are the the 401-404:th forecasted values for y_4. The values seem to oscillate between positive and negative values. The second row of values are the predicted standard errors.

```{r}
# we can submit more than one time series to the ts.plot() function. In this case
# i add, apart from  predicted y, predicty y +- se of prediction
ts.plot(y_4_pred$pred, y_4_pred$pred + y_4_pred$se, 
        y_4_pred$pred - y_4_pred$se, ylab = "Predicted Y", 
        main = expression(paste("Predicted ", Y['1'])), # we can use mathematical notation in r plots!
        lty=c(1:3),
        col = c("blue", "black", "black"))

```

Here we see a graphical plot of the predicted values, combined with the predicted values plus the standard error and minus the standard error. Here we can also see that they seem to "jump", between roughly 0.5 and -1.5.

```{r}
# compare fitted and actual
ts.plot(datf$Y4, y_4_pred$pred, col = c("red", "blue"))
# x & y need to be adjusted manually. A good idea is to take min(x) and add 1 as x - coordinate
# and max y remove 1 as y coordinate. Then fine tune 
legend(x = 401, y = 1.5, legend = c("actual", "fitted"), col = c("red", "blue"), lty=c(1,1))

```

These two plots combine the fitted plot with the actual plot. As we can see, they dont match up very well. There must be something that we have failed to capture - the predicted plot doesn't match the actual plot very well. The actual plot doesnt oscillate in the same way that the fitted model does.

```{r}
### need some measurements of the forecast
y4 <- as.vector(dat$Y4[401:404]) # no real need for these to be time series objects...
y4_hat <- as.vector(y_4_pred$pred) # ... so just store them as your plain vanilla vectors :)

# RMSEA
sqrt(mean((y4-y4_hat)^2))
# MAE 
mean(abs(y4-y4_hat))
# MAPE, not exactly sure whether n = 4 or 404 in the calculations
mape <- 0
for(i in 1:4) {
  mape <- mape + abs((y4[i] - y4_hat[i])/y4[i])
}
100*mean(abs((y4-y4_hat)/y4))
```

Our MAPE (Mean Absolute Percentage Error) is very high, meaning that according to it we have a high amount of error. This strengthens the suspicion that we might have missed something.

### y_5

Here we are going to analyze the dataset y_5. As with the other datasets, we will use the Box-Jenkins approach.

#### Indentification

```{r}
y_5 <- as.ts(datm[ , 7])

## y_5 - AR(1,1)?
layout(matrix(c(1, 1, 2,
                1, 1, 3), nrow=2, byrow=TRUE)) 
ts.plot(y_5, main = "Time Series y_5") # time series plot
acf(y_5, lag.max = 20, type = "correlation", plot = T, main = "ACF") # ACF
acf(y_5, lag.max = 20, type = "partial", plot = T, main = "PACF") # PCAF
par(mfrow = c(1,1)) # set plot window to default

```

This plot is rather choppy, but seems to be somewhat centered around zero. Its ACF seems to geometrically decline. The PACF seems to have a finger, but also seems to decline. It might be an ARMA, since both ACF and PACF seem to decline. If it is an ARMA, then its AR component is probably AR(2). Its MA component looks like an MA(3), since it has 3 "fingers" above zero, although one is close to zero.

#### Estimation

```{r}
### y_5
m5 <- arima(y_5, order = c(2, 0 , 3))
sigma25 <- m5$sigma2

m5
```

#### Evaluation
```{r}
## removing residuals from fitted model
E5 <- residuals(m5)

```


```{r}
## y_5
layout(matrix(c(1, 1, 2,
                1, 1, 3), nrow=2, byrow=TRUE)) 
ts.plot(E5, ylab = "Residuals", col = "blue", main = "Residuals from Fitted Model y_5")
abline(a = mean(E5), b = 0) # adds horizontal line with mean(E5) as intercept and 0 slope
abline(a = mean(E5) + sigma25, b = 0, lty="dotted") # same as above + sigma2
abline(a = mean(E5) - sigma25, b = 0, lty="dotted") # same as above - sigma2

acf(E5, lag.max = 20, type = "correlation", plot = T, main = "ACF") # ACF
acf(E5, lag.max = 20, type = "partial", plot = T, main = "PACF") # PCAF
par(mfrow = c(1,1)) # set plot window to default

```

From the residuals we have extracted, we see no evidance against our assumptions regarding what kind of model we have. Both the ACF and PACF are zero, and the plot is centered around zero.

```{r}
layout(matrix(c(1, 1, 2,
                1, 1, 3), nrow=2, byrow=TRUE)) 
ts.plot(y_5, E5, ylab = "Residuals", col = c("red","black"), main = "Residuals from Fitted Model y_4")
abline(a = mean(E4), b = 0) # adds horizonta2l line with mean(E4) as intercept and 0 slope
abline(a = mean(E4) + sigma24, b = 0, lty="dotted") # same as above + sigma2
abline(a = mean(E4) - sigma24, b = 0, lty="dotted") # same as above - sigma2
```

We can see that the residuals share some things with y_5, but are not distributed in the same way. The residual plot doesnt have the same choppyness, and seem to be more centered around zero, as well as being spread across a smaller interval.

```{r}
qqnorm(E5)
```

The QQ-plot looks approximatly correct, due to the fact that most of the values seem to fall on the theoretical line.

```{r}
(e5_acf <- acf(E5, lag.max = 20, type = "correlation", plot = F))

Box.test(E5, lag = 20)
```

Here we have the correlations for y_5, lags 0-20, as well as a Box-Jenkins test. The results from the test indicate that not all correlations are effectivly zero.

#### Forecast

```{r}
(y_5_pred <- predict(object = m5, n.ahead = 4))

```

Here we see the predicted values for y_5 for the numbers 401-404, with y_5 originally ending at 400. The predicted values seem to start as negative values, at around -0.58, but seem to become less negative with each time point, as t=404 is -0.156. The predicted standard error seems to be very large in comparison with the predicted values.

```{r}
### Predicted y_5
ts.plot(y_5_pred$pred, y_5_pred$pred + y_5_pred$se, 
        y_5_pred$pred - y_5_pred$se, ylab = "Predicted Y", 
        main = expression(paste("Predicted ", Y['5'])), # we can use mathematical notation in r plots!
        lty=c(1:3),
        col = c("blue", "black", "black"))

```

Due to the large size of our standard errors, we end up with a rather large interval. This in turn increases uncertainty. Our interval does not seem to follow our predicted values entirely either, as we can see that they increase faster before time point 402.

```{r}
# compare fitted and actual
ts.plot(datf$Y5, y_5_pred$pred, col = c("red", "blue"))
# x & y need to be adjusted manually. A good idea is to take min(x) and add 1 as x - coordinate
# and max y remove 1 as y coordinate. Then fine tune 
legend(x = 402, y = 0.5, legend = c("actual", "fitted"), col = c("red", "blue"), lty=c(1,1))


```

```{r}
### need some measurements of the forecast
Y_5 <- as.vector(dat$y_5[401:404]) # no real need for these to be time series objects...
y_5_hat <- as.vector(y_5_pred$pred) # ... so just store them as your plain vanilla vectors :)

# RMSEA
sqrt(mean((Y_5-y_5_hat)^2))
# MAE 
mean(abs(Y_5-y_5_hat))
# MAPE, not exactly sure whether n = 4 or 404 in the calculations
mape <- 0
for(i in 1:4) {
  mape <- mape + abs((Y_5[i] - y_5_hat[i])/Y_5[i])
}

100*mean(abs((Y_5-y_5_hat)/Y_5))
```








































































































































# Task 2

## Import dataset

We download the dataset containing quarterly bilateral exchange rate between EU and other currency.

```{r}
dat = read.table(file = "ert_bil_eur_q.tsv", sep = '\t', header = TRUE)
kable(dat[65:70, 1:5], caption = "Quarterly bilateral exchange rate", digits = 2)
```

And then we only use the quarterly bilateral exchange rate between EU and SEK.

```{r}
dat = dat %>% 
  filter(statinfo.unit.currency.time == "END,NAC,SEK") %>% 
  select(-statinfo.unit.currency.time)
kable(dat[,1:9], caption = "Quarterly bilateral exchange rate between EU and SEK (Head 9)", digits = 2)
dat = t(dat)
y = dat[,1]
y = as.numeric(y)
y = y[!is.na(y)]
```

We "save" 5 observations for forecast evaluation.

```{r}
y_m = y[1:175]
y_f = y[176:181]
```

The first step that is to determine what type of process that generated my observed 190 observations.

## Identify the model

```{r}
# split up plot window
layout(matrix(c(1, 1, 2,
1, 1, 3), nrow=2, byrow=TRUE))
ts.plot(y_m, main = "Time Series") # time series plot, plot 1 in matrix argument
acf(y_m, lag.max = 20, type = "correlation", plot = T, main = "ACF") # ACF, plot 2 in matrix argument
acf(y_m, lag.max = 20, type = "partial", plot = T, main = "PACF") # PCAF, plot 3 in matrix argument
```


## Estimate the model

## Do the diagnostics

## Forecast




